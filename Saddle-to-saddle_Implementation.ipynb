{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Saddle-to-Saddle Dynamics in Diagonal Linear Networks<br>\n",
    "*Pesme, S., & Flammarion, N. (2024). Saddle-to-saddle dynamics in diagonal linear networks*. Journal of Statistical Mechanics Theory and Experiment, 2024(10), 104016. https://doi.org/10.1088/1742-5468/ad65e3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Button, VBox\n",
    "import math\n",
    "from itertools import product, combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation and Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # Dimensions of the input vectors\n",
    "n = 2 # Number of samples\n",
    "\n",
    "# Initial values of the weights \n",
    "# alpha = 1e-2\n",
    "# v_0 = np.array([0 for x in range(d)])\n",
    "# u_0 = math.sqrt(2)*alpha*np.array([1 for x in range(d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a dataset for our use. The method of generation is taken from Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 1.5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dataset(n, d):\n",
    "    # Generate the random parameters for the dataset\n",
    "    H = np.diag(np.random.rand(d))  # Random diagonal entries for covariance matrix\n",
    "    beta_star = np.random.randn(d) # Initialize beta^* as a random vector in R^d\n",
    "    \n",
    "    # Generate the dataset\n",
    "    x = np.random.multivariate_normal(np.zeros(d), H, n) # Generate x_i from multivariate normal distribution with mean 0 and covariance H\n",
    "    y = np.dot(x, beta_star)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Generate the dataset for fixed H and beta_star\n",
    "def generate_dataset_fixed_H_beta(n,d,H,beta_star):\n",
    "    # Generate the dataset\n",
    "    x = np.random.multivariate_normal(np.zeros(d), H, n) # Generate x_i from multivariate normal distribution with mean 0 and covariance H\n",
    "    y = np.dot(x, beta_star)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# samples_x, samples_y = generate_dataset(n, d)\n",
    "# For the sake of consistent results when testing, we will fix the dataset\n",
    "\n",
    "# Possible TODO: \n",
    "#   Calculate the saddle points of the loss F to always predict the saddle points \n",
    "#   Take a dataset and replicate the illustrative example in 3.1 with the values to see if it matches similarly\n",
    "\n",
    "# This dataset gives a weird bump on Beta[0] at the beginning, grows as alpha is larger - Why?\n",
    "# One jump Beta[0] -> -0.5\n",
    "# Two jumps Beta[1] -> 1.6 -> 2\n",
    "samples_x = np.array([[1, 0.6], [0.2, 0.8]])\n",
    "beta_star = np.array([-0.5, 2])\n",
    "\n",
    "# # Large values break due to some overflow - Take a closer look at why, may be a computer accuracy issue?\n",
    "# samples_x = np.array([[20, 15], [2, 50]])\n",
    "# beta = np.array([1.5, 20])\n",
    "\n",
    "# # Converges incredibly fast, time <50 for any alpha value on the sliders\n",
    "# # This means that the conditions K1-K4 are quickly limiting\n",
    "# # UNDERSTANDING SO FAR: The time is affected by the gradient of loss L, the values of x and y affect this, in this case we have a 'large' y[1]=8 value\n",
    "# samples_x = np.array([[0.5, 0.1], [-0.2, 1]])\n",
    "# beta = np.array([-5, 7])\n",
    "\n",
    "\n",
    "\n",
    "samples_y = samples_x @ beta_star\n",
    "\n",
    "samples_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Important Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer Diagonal Linear Network\n",
    "$ x \\mapsto \\langle u, \\sigma(diag(v)x)\\rangle $ <br> \n",
    "$x \\mapsto \\langle \\beta, x\\rangle$ where $\\beta = u \\odot v$\n",
    "\n",
    "$u$ - Output weights <br> \n",
    "$\\sigma$ - Identity function<br> \n",
    "$\\text{diag}(v)$ - Inner weights<br> \n",
    "\n",
    "\n",
    "### Quadratic Loss Function\n",
    "$L(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\langle \\beta, x_i\\rangle - y_i)^2$<br>\n",
    "$F(\\omega) := L(u \\odot v)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Layer Diagonal Linear Network\n",
    "def twolDLN(u,v,x):\n",
    "    return np.inner(u*v, x) # We use the regression vector here for conciseness\n",
    "\n",
    "# 2-Layer Diagonal Linear Network with beta\n",
    "def twolDLN_beta(beta,x):\n",
    "    return np.inner(beta, x) \n",
    "\n",
    "# Quadratic Loss Function\n",
    "def loss(u,v):\n",
    "    return (1/2*n)*sum([(twolDLN(u,v,samples_x[i])-samples_y[i])**2 for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential $\\phi_\\alpha$ - Hyperbolic Entropy function:\n",
    "$\\phi_\\alpha(\\Beta) = \\frac{1}{2} \\sum^d_{i=1}\\left( \\Beta_i \\sinh^{-1}(\\frac{\\Beta_i}{\\alpha^2} - \\sqrt{\\Beta_i^2 +\\alpha^4} + \\alpha^2)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential(beta, alpha):\n",
    "    # Ensure beta is a numpy array\n",
    "    beta = np.array(beta)\n",
    "    \n",
    "    # Calculate the function for each beta_i\n",
    "    result = 1/2*np.sum([beta[i] * np.arcsinh(beta[i] / alpha**2) - np.sqrt(beta[i]**2 + alpha**4) + alpha**2 for i in range(d)])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Rescaling for the potential\n",
    "\n",
    "$\\tilde{\\phi}_\\alpha := \\frac{1}{\\ln(1/\\alpha)} \\phi_\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_potential(beta, alpha):\n",
    "\n",
    "    # Rescale the hyperbolic entropy\n",
    "    result = 1/(math.log(1/alpha)) * potential(beta, alpha)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimise loss $F$ (without accelerating time)\n",
    "\n",
    "$\\text{d}w_t = - \\nabla F(w_t)\\text{d}t$\n",
    "\n",
    "For Gradient descent:\n",
    "$w_{t+1} = w_t - \\eta \\nabla F(w_t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation $\\nabla F$\n",
    "Consider for now only $u$\n",
    "\n",
    "$\\frac{\\partial F}{\\partial u} = \\frac{1}{2n} \\sum_{i=1}^n 2(\\langle u \\odot v,x_i \\rangle - y_i) \\cdot \\frac{\\partial}{\\partial u}\\langle u \\odot v, x_i \\rangle$\n",
    "\n",
    "Since $\\langle u \\odot v, x_i\\rangle = \\sum_{j=1}^d (u_j v_j x_{ij})$ then $\\frac{\\partial}{\\partial u_j}\\langle u \\odot v, x_i\\rangle = (v_j x_{ij})$\n",
    "\n",
    "Therefore\n",
    "$\\frac{\\partial F}{\\partial u} = \\frac{1}{n} \\sum_{i=1}^n (\\langle u \\odot v,x_i \\rangle - y_i) \\cdot (v \\odot x_i )$\n",
    "\n",
    "Similarly \n",
    "$\\frac{\\partial F}{\\partial v} = \\frac{1}{n} \\sum_{i=1}^n (\\langle u \\odot v,x_i \\rangle - y_i) \\cdot (u \\odot x_i )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Derivative of the Loss Function with respect to w\n",
    "def nabla_F(u,v):\n",
    "    nabla_F_u = 1/n * sum([(twolDLN(u,v,samples_x[i])-samples_y[i])*(v*samples_x[i]) for i in range(n)])\n",
    "    nabla_F_v = 1/n * sum([(twolDLN(u,v,samples_x[i])-samples_y[i])*(u*samples_x[i]) for i in range(n)])\n",
    "    return nabla_F_u, nabla_F_v, np.concatenate((nabla_F_u, nabla_F_v))\n",
    "\n",
    "def nabla_L(beta):\n",
    "    nabla_L = 1/n * sum([(twolDLN_beta(beta,samples_x[i])-samples_y[i])*samples_x[i] for i in range(n)])\n",
    "    return nabla_L\n",
    "\n",
    "# Gradient Descent on F\n",
    "def gradient_descent_F(u,v,eta):\n",
    "    nabla_F_u, nabla_F_v, _ = nabla_F(u,v)\n",
    "    u = u - eta*nabla_F_u\n",
    "    v = v - eta*nabla_F_v\n",
    "    return u, v\n",
    "\n",
    "# Gradient Descent on L\n",
    "def gradient_descent_L(beta,eta):\n",
    "    nabla_beta = nabla_L(beta)\n",
    "    beta = beta - eta*nabla_beta\n",
    "    return beta\n",
    "\n",
    "# Scaled Gradient Descent on L\n",
    "def gradient_descent_L_scaled(alpha,beta,eta):\n",
    "    nabla_beta = nabla_L(beta)\n",
    "    beta = beta - eta* math.log(1/alpha)*nabla_beta\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Gradient Flow\n",
    "\n",
    "We are interested in trying to observe the saddle-to-saddle movements that we may expect.\n",
    "$\\beta$ is the parameter that we pay attention to\n",
    "\n",
    "Things that we expect to notice:\n",
    "1. As the $\\alpha$ approaches 0, the time required to notice the saddle 'jumps' increases (the time taken for a jump remains the same)\n",
    "1. When $\\alpha$ is very close to 0, then $\\Beta$ remains at the origin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d88194aea54d789e0676930ce482dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=200, description='Time', max=1000, min=50, step=50), FloatSlider(value=0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gradient_descent(time=200, eta=0.1, alpha=0.1)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following code is used to plot the gradient descent path of beta over time for the 2-layer diagonal linear network.\n",
    "It will only work for d=2, as the plot is in 2D.\n",
    "\"\"\"\n",
    "\n",
    "def plot_gradient_descent(time=200,eta=0.1,alpha=1e-1):\n",
    "    # Number of steps for the gradient descent1\n",
    "    n_steps = math.floor(time / eta)\n",
    "\n",
    "    # Initial values of the weights\n",
    "    v_0 = np.array([0 for x in range(d)])\n",
    "    u_0 = math.sqrt(2)*alpha*np.array([1 for x in range(d)])\n",
    "\n",
    "    # Gradient Descent\n",
    "    u = u_0\n",
    "    v = v_0\n",
    "    u_values = [u] # Store the values of u for each iteration\n",
    "    v_values = [v] # Store the values of v for each iteration\n",
    "    beta_values = [v*u] # Store the values of beta for each iteration\n",
    "    beta_values_L = [v*u] # Store the values of beta for each iteration\n",
    "    potential_values = [potential(v*u, alpha)] # Store the values of the potential for each iteration # TODO: Check if this is at all relevant\n",
    "    scaled_potential_values = [scaled_potential(v*u,alpha)] # Store the values of the hyperbolic entropy for each iteration\n",
    "\n",
    "    # Perform the gradient descent for beta on the loss F\n",
    "    for _ in range(n_steps):\n",
    "        u, v = gradient_descent_F(u,v,eta)\n",
    "        u_values.append(u.copy()) # Save values for plotting\n",
    "        v_values.append(v.copy()) # Save values for plotting\n",
    "        beta_values.append(v*u)\n",
    "        #potential_values.append(potential(v*u, alpha))\n",
    "\n",
    "    # Perform the gradient descent for beta on the loss L\n",
    "    beta = v_0*u_0\n",
    "    for _ in range(n_steps):\n",
    "        beta = gradient_descent_L(beta,eta)\n",
    "        #beta_values_L.append(beta)\n",
    "        potential_values.append(potential(beta, alpha))\n",
    "\n",
    "    # Perform the scaled gradient descent for beta on the loss L\n",
    "    beta = v_0*u_0\n",
    "    for _ in range(n_steps):\n",
    "        beta = gradient_descent_L_scaled(alpha,beta,eta)\n",
    "        #beta_values_L.append(beta)\n",
    "        scaled_potential_values.append(scaled_potential(beta, alpha))\n",
    "\n",
    "    # Convert u_values to a numpy array for easier plotting\n",
    "    u_values = np.array(u_values)\n",
    "    v_values = np.array(v_values)\n",
    "    beta_values = np.array(beta_values)\n",
    "    time_values = np.array([step*eta for step in range(n_steps+1)])\n",
    "    \n",
    "    # for beta in beta_values:\n",
    "    #     scaled_potential_values.append(scaled_potential(beta, alpha))\n",
    "\n",
    "\n",
    "    ## PLOTTING FUN\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(24, 12))\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    axs[0, 0].scatter(time_values, beta_values[:, 0], s=0.2, c='purple')\n",
    "    axs[0, 0].set_title(f'Beta[0] over time (eta = {round(eta,5)}, alpha = {round(alpha,5)})')\n",
    "    axs[0, 0].set_xlabel('Time')\n",
    "    axs[0, 0].set_ylabel('Beta[0]')\n",
    "    axs[0,0].grid(True)\n",
    "    axs[0,0].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "    axs[0,0].minorticks_on()\n",
    "\n",
    "    axs[0, 1].scatter(time_values, beta_values[:, 1], s=0.2, c='purple')\n",
    "    axs[0, 1].set_title(f'Beta[1] over time (eta = {round(eta,5)}, alpha = {round(alpha,5)})')\n",
    "    axs[0, 1].set_xlabel('Time')\n",
    "    axs[0, 1].set_ylabel('Beta[1]')\n",
    "    axs[0,1].grid(True)\n",
    "    axs[0,1].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "    axs[0,1].minorticks_on()\n",
    "\n",
    "    axs[1, 0].scatter(time_values, potential_values, s=0.2, c='red')\n",
    "    axs[1, 0].set_title(f'Potential over time (eta = {round(eta,5)}, alpha = {round(alpha,5)})')\n",
    "    axs[1, 0].set_xlabel('Time')\n",
    "    axs[1, 0].set_ylabel('Potential')\n",
    "    axs[1,0].grid(True)\n",
    "    axs[1,0].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "    axs[1,0].minorticks_on()\n",
    "\n",
    "    axs[1, 1].scatter(time_values, scaled_potential_values, s=0.2, c='red')\n",
    "    axs[1, 1].set_title(f'Scaled Potential over time (eta = {round(eta,5)}, alpha = {round(alpha,5)})')\n",
    "    axs[1, 1].set_xlabel('Time')\n",
    "    axs[1, 1].set_ylabel('Scaled Potential')\n",
    "    axs[1,1].grid(True)\n",
    "    axs[1,1].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "    axs[1,1].minorticks_on()\n",
    "\n",
    "    # Create a button widget\n",
    "    save_button = Button(description=\"Save Plot\", button_style=\"success\")  # Green button\n",
    "\n",
    "    # Display the button below the plot\n",
    "    display(VBox([save_button]))\n",
    "    plot = plt.gcf()\n",
    "    plt.show()\n",
    "\n",
    "        # Save the plot on button click\n",
    "    def save_plot(p):\n",
    "        filename = f'plots/Beta[{beta_star[0]},{beta_star[1]}]_Plots_{round(alpha, 5)}_{time}_{round(eta, 5)}.png'  \n",
    "        fig.savefig(filename)\n",
    "\n",
    "    save_button.on_click(save_plot) \n",
    "\n",
    "# Add sliders to control the parameters\n",
    "interact(\n",
    "    plot_gradient_descent,\n",
    "    time=IntSlider(value=200, min=50, max=1000, step=50, description=\"Time\"),\n",
    "    eta=FloatSlider(value=0.01, min=0.001, max=0.1, step=0.001, description=\"Learning Rate\",readout_format='.3f'),\n",
    "    alpha=FloatSlider(value=0.00001, min=0.0001, max=0.1, step=0.0001, description=\"Alpha\",readout_format='.4f'),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(Assumption 1) - General Positions**\n",
    "\n",
    "For any $k \\leq \\min(n,d)$ and arbitrary signs $\\sigma_1, \\dots, \\sigma_k \\in \\{-1,1\\}$, the affine span of any $k$ points $\\sigma \\tilde{x}_{j_1}, \\ldots, \\sigma \\tilde{x}_{j_k}$ does not contain any element of the set $\\{ \\pm \\tilde{x_j}, j \\neq j_1, \\ldots, j_k\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_affine_span(X, k):\n",
    "    \"\"\"\n",
    "    Check if the dataset X satisfies the general position assumption.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy array): Dataset with shape (n, d), where n is the number of samples and d is the dimensionality.\n",
    "                     The ith column of X corresponds to the feature vector x_̃j in the assumption.\n",
    "    k (int): Number of points to check in the affine span. Must be <= min(n, d).\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the assumption holds, False otherwise.\n",
    "\n",
    "    Warning: \n",
    "    This function has exponential complexity in d and k. It is only suitable for small values of d and k.\n",
    "    \"\"\"\n",
    "    # Generate all combinations of k columns and their corresponding sign combinations\n",
    "    for indices in combinations(range(d), k): # all combinations of k columns\n",
    "        for signs in product([-1, 1], repeat=k): # all sign combinations\n",
    "            # Select k columns (corresponding to features) and apply sign changes\n",
    "            selected_columns = np.array([sign * X[:, idx] for sign, idx in zip(signs, indices)]).T  # Make array of all selected columns and their signs\n",
    "            \n",
    "            # Compute the affine span of the selected columns\n",
    "            # (Affine span is equivalent to checking linear independence after centering the points)\n",
    "            centered_columns = selected_columns - selected_columns.mean(axis=0) #Centering is necessary for checking the #affine# span not just the linear span\n",
    "            if np.linalg.matrix_rank(centered_columns) < k:\n",
    "                continue  # If points are linearly dependent, skip this combination\n",
    "            \n",
    "            # Now check if any other columns lie in the affine span\n",
    "            remaining_indices = [i for i in range(d) if i not in indices] # Columns not in the selected set\n",
    "            for idx in remaining_indices:\n",
    "                for sign in [-1, 1]:\n",
    "                    candidate_column = sign * X[:, idx]\n",
    "                    augmented_matrix = np.hstack([centered_columns, candidate_column[:, None] - selected_columns.mean(axis=0)])\n",
    "                    if np.linalg.matrix_rank(augmented_matrix) == k:\n",
    "                        # If the rank doesn't increase, the point lies in the affine span\n",
    "                        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General position assumption is satisfied for k =  1\n",
      "General position assumption is satisfied for k =  2\n"
     ]
    }
   ],
   "source": [
    "k = min(d, n)  # Minimum of d and n for general positions\n",
    "\n",
    "for i in range(1, k+1):\n",
    "    if not check_affine_span(samples_x, i):\n",
    "        print(\"General position assumption is not satisfied for k = \", i)\n",
    "    else:\n",
    "        print(\"General position assumption is satisfied for k = \", i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
