{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Saddle-to-Saddle Dynamics in Diagonal Linear Networks<br>\n",
    "*Pesme, S., & Flammarion, N. (2024). Saddle-to-saddle dynamics in diagonal linear networks*. Journal of Statistical Mechanics Theory and Experiment, 2024(10), 104016. https://doi.org/10.1088/1742-5468/ad65e3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Button, VBox\n",
    "import math\n",
    "from itertools import product, combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation and Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # Dimensions of the input vectors\n",
    "n = 2 # Number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a dataset for our use. The method of generation is taken from Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n, d):\n",
    "    # Generate the random parameters for the dataset\n",
    "    H = np.diag(np.random.rand(d))  # Random diagonal entries for covariance matrix\n",
    "    beta_star = np.random.randn(d) # Initialize beta^* as a random vector in R^d\n",
    "    \n",
    "    # Generate the dataset\n",
    "    x = np.random.multivariate_normal(np.zeros(d), H, n) # Generate x_i from multivariate normal distribution with mean 0 and covariance H\n",
    "    y = np.dot(x, beta_star)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Generate the dataset for fixed H and beta_star\n",
    "def generate_dataset_fixed_H_beta(n,d,H,beta_star):\n",
    "    # Generate the dataset\n",
    "    x = np.random.multivariate_normal(np.zeros(d), H, n) # Generate x_i from multivariate normal distribution with mean 0 and covariance H\n",
    "    y = np.dot(x, beta_star)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# samples_x, samples_y = generate_dataset(n, d)\n",
    "# For the sake of consistent results when testing, we will fix the dataset\n",
    "\n",
    "# Possible TODO: \n",
    "#   Calculate the saddle points of the loss F to always predict the saddle points \n",
    "#   Take a dataset and replicate the illustrative example in 3.1 with the values to see if it matches similarly\n",
    "\n",
    "# This dataset gives a weird bump on Beta[0] at the beginning, grows as alpha is larger - Why?\n",
    "# One jump Beta[0] -> -0.5\n",
    "# Two jumps Beta[1] -> 1.6 -> 2\n",
    "# samples_x = np.array([[1, 0.6], [0.2, 0.8]])\n",
    "# beta_star = np.array([-0.5, 2])\n",
    "\n",
    "# # Large values break due to some overflow - Take a closer look at why, may be a computer accuracy issue?\n",
    "# samples_x = np.array([[20, 15], [2, 50]])\n",
    "# beta_star = np.array([1.5, 20])\n",
    "\n",
    "# # Converges incredibly fast, time <50 for any alpha value on the sliders\n",
    "# # This means that the conditions K1-K4 are quickly limiting\n",
    "# # UNDERSTANDING SO FAR: The time is affected by the gradient of loss L, the values of x and y affect this, in this case we have a 'large' y[1]=8 value\n",
    "# samples_x = np.array([[0.5, 0.1], [-0.2, 1]])\n",
    "# beta_star = np.array([-5, 7])\n",
    "\n",
    "# samples_x = np.array([[0.8, 0.3], [0.1, 0.64]])\n",
    "# beta_star = np.array([-1.5, 1.4])\n",
    "\n",
    "# You can see the effect of time scaling much more clearly here\n",
    "# samples_x = np.array([[0.4, 0.12], [0.13, 0.2]])\n",
    "# beta_star = np.array([-0.2, 2])\n",
    "\n",
    "samples_x = np.array([[1, 0.2], [0, math.sqrt(0.1-0.2**2)]])\n",
    "beta_star = np.array([-0.2, 2])\n",
    "\n",
    "\n",
    "samples_y = samples_x @ beta_star\n",
    "\n",
    "samples_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Important Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer Diagonal Linear Network\n",
    "$ x \\mapsto \\langle u, \\sigma(diag(v)x)\\rangle $ <br> \n",
    "$x \\mapsto \\langle \\beta, x\\rangle$ where $\\beta = u \\odot v$\n",
    "\n",
    "$u$ - Output weights <br> \n",
    "$\\sigma$ - Identity function<br> \n",
    "$\\text{diag}(v)$ - Inner weights<br> \n",
    "\n",
    "\n",
    "### Quadratic Loss Function\n",
    "$L(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\langle \\beta, x_i\\rangle - y_i)^2$<br>\n",
    "$F(\\omega) := L(u \\odot v)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Layer Diagonal Linear Network\n",
    "def twolDLN(u,v,x):\n",
    "    return np.inner(u*v, x) # We use the regression vector here for conciseness\n",
    "\n",
    "# 2-Layer Diagonal Linear Network with beta\n",
    "def twolDLN_beta(beta,x):\n",
    "    return np.inner(beta, x) \n",
    "\n",
    "# Quadratic Loss Function\n",
    "def loss(u,v):\n",
    "    return (1/2*n)*sum([(twolDLN(u,v,samples_x[i])-samples_y[i])**2 for i in range(n)])\n",
    "\n",
    "def loss_L_beta(beta):\n",
    "    return (1/2*n)*sum([(twolDLN_beta(beta,samples_x[i])-samples_y[i])**2 for i in range(n)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential $\\phi_\\alpha$ - Hyperbolic Entropy function:\n",
    "$\\phi_\\alpha(\\Beta) = \\frac{1}{2} \\sum^d_{i=1}\\left( \\Beta_i \\sinh^{-1}(\\frac{\\Beta_i}{\\alpha^2} - \\sqrt{\\Beta_i^2 +\\alpha^4} + \\alpha^2)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential(beta, alpha):\n",
    "    # Ensure beta is a numpy array\n",
    "    beta = np.array(beta)\n",
    "    \n",
    "    # Calculate the function for each beta_i\n",
    "    result = 1/2*np.sum([beta[i] * np.arcsinh(beta[i] / alpha**2) - np.sqrt(beta[i]**2 + alpha**4) + alpha**2 for i in range(d)])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Rescaling for the potential\n",
    "\n",
    "$\\tilde{\\phi}_\\alpha := \\frac{1}{\\ln(1/\\alpha)} \\phi_\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_potential(beta, alpha):\n",
    "\n",
    "    # Rescale the hyperbolic entropy\n",
    "    result = 1/(math.log(1/alpha)) * potential(beta, alpha)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimise loss $F$ (without accelerating time)\n",
    "\n",
    "$\\text{d}w_t = - \\nabla F(w_t)\\text{d}t$\n",
    "\n",
    "For Gradient descent:\n",
    "$w_{t+1} = w_t - \\eta \\nabla F(w_t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation $\\nabla F$\n",
    "Consider for now only $u$\n",
    "\n",
    "$\\frac{\\partial F}{\\partial u} = \\frac{1}{2n} \\sum_{i=1}^n 2(\\langle u \\odot v,x_i \\rangle - y_i) \\cdot \\frac{\\partial}{\\partial u}\\langle u \\odot v, x_i \\rangle$\n",
    "\n",
    "Since $\\langle u \\odot v, x_i\\rangle = \\sum_{j=1}^d (u_j v_j x_{ij})$ then $\\frac{\\partial}{\\partial u_j}\\langle u \\odot v, x_i\\rangle = (v_j x_{ij})$\n",
    "\n",
    "Therefore\n",
    "$\\frac{\\partial F}{\\partial u} = \\frac{1}{n} \\sum_{i=1}^n (\\langle u \\odot v,x_i \\rangle - y_i) \\cdot (v \\odot x_i )$\n",
    "\n",
    "Similarly \n",
    "$\\frac{\\partial F}{\\partial v} = \\frac{1}{n} \\sum_{i=1}^n (\\langle u \\odot v,x_i \\rangle - y_i) \\cdot (u \\odot x_i )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Derivative of the Loss Function with respect to w\n",
    "def nabla_F(u,v):\n",
    "    nabla_F_u = 1/n * sum([(twolDLN(u,v,samples_x[i])-samples_y[i])*(v*samples_x[i]) for i in range(n)])\n",
    "    nabla_F_v = 1/n * sum([(twolDLN(u,v,samples_x[i])-samples_y[i])*(u*samples_x[i]) for i in range(n)])\n",
    "    return nabla_F_u, nabla_F_v, np.concatenate((nabla_F_u, nabla_F_v))\n",
    "\n",
    "def nabla_L(beta):\n",
    "    nabla_L = 1/n * sum([(twolDLN_beta(beta,samples_x[i])-samples_y[i])*samples_x[i] for i in range(n)])\n",
    "    return nabla_L\n",
    "\n",
    "# Gradient Descent on F\n",
    "def gradient_descent_F(u,v,eta):\n",
    "    nabla_F_u, nabla_F_v, _ = nabla_F(u,v)\n",
    "    u = u - eta*nabla_F_u\n",
    "    v = v - eta*nabla_F_v\n",
    "    return u, v\n",
    "\n",
    "# Gradient Descent on L\n",
    "def gradient_descent_L(beta,eta):\n",
    "    nabla_beta = nabla_L(beta)\n",
    "    beta = beta - eta*nabla_beta\n",
    "    return beta\n",
    "\n",
    "# Scaled Gradient Descent on L\n",
    "def gradient_descent_L_scaled(alpha,beta,eta):\n",
    "    nabla_beta = nabla_L(beta)\n",
    "    beta = beta - eta* math.log(1/alpha)*nabla_beta\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Gradient Flow\n",
    "\n",
    "We are interested in trying to observe the saddle-to-saddle movements that we may expect.\n",
    "$\\beta$ is the parameter that we pay attention to\n",
    "\n",
    "Things that we expect to notice:\n",
    "1. As the $\\alpha$ approaches 0, the time required to notice the saddle 'jumps' increases (the time taken for a jump remains the same)\n",
    "1. When $\\alpha$ is very close to 0, then $\\Beta$ remains at the origin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code is used to plot the gradient descent path of beta over time for the 2-layer diagonal linear network.\n",
    "It will only work for d=2, as the plot is in 2D.\n",
    "\"\"\"\n",
    "\n",
    "def plot_gradient_descent(time=200,eta=0.1,alpha=1e-1):\n",
    "    # Number of steps for the gradient descent1\n",
    "    n_steps = math.floor(time / eta)\n",
    "\n",
    "    # Initial values of the weights\n",
    "    v_0 = np.array([0 for x in range(d)])\n",
    "    u_0 = math.sqrt(2)*alpha*np.array([1 for x in range(d)])\n",
    "\n",
    "    # Gradient Descent\n",
    "    u = u_0\n",
    "    v = v_0\n",
    "    u_values = [u] # Store the values of u for each iteration\n",
    "    v_values = [v] # Store the values of v for each iteration\n",
    "    beta_values = [v*u] # Store the values of beta for each iteration\n",
    "    potential_values = [potential(v*u, alpha)] # Store the values of the potential for each iteration # TODO: Check if this is at all relevant\n",
    "\n",
    "    # Perform the gradient descent for beta on the loss F\n",
    "    for _ in range(n_steps):\n",
    "        u, v = gradient_descent_F(u,v,eta)\n",
    "        u_values.append(u.copy()) # Save values for plotting\n",
    "        v_values.append(v.copy()) # Save values for plotting\n",
    "        beta_values.append(v*u)\n",
    "        potential_values.append(potential(v*u, alpha))\n",
    "\n",
    "    # Convert u_values to a numpy array for easier plotting\n",
    "    u_values = np.array(u_values)\n",
    "    v_values = np.array(v_values)\n",
    "    beta_values = np.array(beta_values)\n",
    "    time_values = np.array([step*eta for step in range(n_steps+1)])\n",
    "\n",
    "    # PLOTTING FUN\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(24, 12))\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    axs[0].scatter(time_values, beta_values[:, 0], s=0.2, c='purple')\n",
    "    axs[0].set_title(f'Beta[0] over time (eta = {round(eta,5)}, alpha = {round(alpha,11)})')\n",
    "    axs[0].set_xlabel('Time')\n",
    "    axs[0].set_ylabel('Beta[0]')\n",
    "    axs[0].grid(True)\n",
    "    axs[0].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "    axs[0].minorticks_on()\n",
    "\n",
    "    axs[1].scatter(time_values, beta_values[:, 1], s=0.2, c='purple')\n",
    "    axs[1].set_title(f'Beta[1] over time (eta = {round(eta,5)}, alpha = {round(alpha,11)})')\n",
    "    axs[1].set_xlabel('Time')\n",
    "    axs[1].set_ylabel('Beta[1]')\n",
    "    axs[1].grid(True)\n",
    "    axs[1].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "    axs[1].minorticks_on()\n",
    "\n",
    "    # Create a button widget\n",
    "    save_button = Button(description=\"Save Plot\", button_style=\"success\")  # Green button\n",
    "\n",
    "    # Display the button below the plot\n",
    "    display(VBox([save_button]))\n",
    "    plot = plt.gcf()\n",
    "    plt.show()\n",
    "\n",
    "        # Save the plot on button click\n",
    "    def save_plot(p):\n",
    "        filename = f'plots/Beta[{beta_star[0]},{beta_star[1]}]_Plots_{round(alpha, 5)}_{time}_{round(eta, 5)}.png'  \n",
    "        fig.savefig(filename)\n",
    "\n",
    "    save_button.on_click(save_plot) \n",
    "\n",
    "# Add sliders to control the parameters\n",
    "interact(\n",
    "    plot_gradient_descent,\n",
    "    time=IntSlider(value=200, min=50, max=1000, step=50, description=\"Time\"),\n",
    "    eta=FloatSlider(value=0.01, min=0.001, max=0.1, step=0.001, description=\"Learning Rate\",readout_format='.3f'),\n",
    "    alpha=FloatSlider(value=0.1, min=1e-8, max=0.1, step=1e-8, description=\"Alpha\",readout_format='.10f'),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(time=200,eta=0.1,alpha=1e-1):\n",
    "    # Number of steps for the gradient descent1\n",
    "    n_steps = math.floor(time / eta)\n",
    "\n",
    "    # Initial values of the weights\n",
    "    v_0 = np.array([0 for x in range(d)])\n",
    "    u_0 = math.sqrt(2)*alpha*np.array([1 for x in range(d)])\n",
    "\n",
    "    # Gradient Descent\n",
    "    u = u_0\n",
    "    v = v_0\n",
    "    u_values = [u] # Store the values of u for each iteration\n",
    "    v_values = [v] # Store the values of v for each iteration\n",
    "    beta_values = [v*u] # Store the values of beta for each iteration\n",
    "    potential_values = [potential(v*u, alpha)] # Store the values of the potential for each iteration # TODO: Check if this is at all relevant\n",
    "\n",
    "    # Perform the gradient descent for beta on the loss F\n",
    "    for _ in range(n_steps):\n",
    "        u, v = gradient_descent_F(u,v,eta)\n",
    "        u_values.append(u.copy()) # Save values for plotting\n",
    "        v_values.append(v.copy()) # Save values for plotting\n",
    "        beta_values.append(v*u)\n",
    "        potential_values.append(potential(v*u, alpha))\n",
    "\n",
    "    # Convert u_values to a numpy array for easier plotting\n",
    "    u_values = np.array(u_values)\n",
    "    v_values = np.array(v_values)\n",
    "    beta_values = np.array(beta_values)\n",
    "    time_values = np.array([step*eta for step in range(n_steps+1)])\n",
    "\n",
    "    return beta_values, time_values\n",
    "\n",
    "beta_1, time_1 = gradient_descent(time=3000, eta=0.01, alpha=1e-8)\n",
    "beta_2, time_2 = gradient_descent(time=2000, eta=0.01, alpha=1e-2)\n",
    "beta_3, time_3 = gradient_descent(time=1000, eta=0.01, alpha=0.1)\n",
    "beta_4, time_4 = gradient_descent(time=2000, eta=0.01, alpha=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(24, 12))\n",
    "plt.figure(figsize=(24, 12))\n",
    "axs[0].scatter(time_1*(1/math.log(1/1e-8)), beta_1[:, 0], s=0.2, c='purple', label='Alpha = 1e-8')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Beta[0]')\n",
    "axs[0].grid(True)\n",
    "axs[0].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[0].minorticks_on()\n",
    "\n",
    "axs[1].scatter(time_1*(1/math.log(1/1e-8)), beta_1[:, 1], s=0.2, c='purple', label='Alpha = 1e-8')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Beta[1]')\n",
    "axs[1].grid(True)\n",
    "axs[1].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[1].minorticks_on()\n",
    "\n",
    "axs[0].scatter(time_4*(1/math.log(1/1e-4)), beta_4[:, 0], s=0.22, c='blue', label='Alpha = 1e-4')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Beta[0]')\n",
    "axs[0].grid(True)\n",
    "axs[0].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[0].minorticks_on()\n",
    "\n",
    "axs[1].scatter(time_4*(1/math.log(1/1e-4)), beta_4[:, 1], s=0.2, c='blue', label='Alpha = 1e-4')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Beta[1]')\n",
    "axs[1].grid(True)\n",
    "axs[1].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[1].minorticks_on()\n",
    "\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(24, 12))\n",
    "# plt.figure(figsize=(24, 12))\n",
    "axs[0].scatter(time_2*(1/math.log(1/1e-2)), beta_2[:, 0], s=0.2, c='green', label='Alpha = 1e-2')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Beta[0]')\n",
    "axs[0].grid(True)\n",
    "axs[0].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[0].minorticks_on()\n",
    "\n",
    "axs[1].scatter(time_2*(1/math.log(1/1e-2)), beta_2[:, 1], s=0.2, c='green', label='Alpha = 1e-2')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Beta[1]')\n",
    "axs[1].grid(True)\n",
    "axs[1].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[1].minorticks_on()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(24, 12))\n",
    "# plt.figure(figsize=(24, 12))\n",
    "axs[0].scatter(time_3*(1/math.log(1/0.1)), beta_3[:, 0], s=0.2, c='red', label='Alpha = 0.1')\n",
    "axs[0].set_xlabel('Time')\n",
    "axs[0].set_ylabel('Beta[0]')\n",
    "axs[0].grid(True)\n",
    "axs[0].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[0].minorticks_on()\n",
    "\n",
    "axs[1].scatter(time_3*(1/math.log(1/0.1)), beta_3[:, 1], s=0.2, c='red', label='Alpha = 0.1')\n",
    "axs[1].set_xlabel('Time')\n",
    "axs[1].set_ylabel('Beta[1]')\n",
    "axs[1].grid(True)\n",
    "axs[1].grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "axs[1].minorticks_on()\n",
    "\n",
    "axs[0].set_xlim(0, 100)\n",
    "axs[1].set_xlim(0, 100)\n",
    "\n",
    "axs[0].legend(fontsize = 15,markerscale=10)\n",
    "axs[1].legend(fontsize = 15,markerscale=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "time_1.shape, beta_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.scatter(time_1*(1/math.log(1/1e-8)), beta_1[:, 0], s=0.2, c='red', label='Alpha = 1e-30')\n",
    "plt.grid(True)\n",
    "plt.grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.scatter(time_1*(1/math.log(1/1e-8)), beta_1[:, 1], s=0.2, c='blue')\n",
    "plt.grid(True)\n",
    "plt.grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.xlim(0,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO - The algorithm implementation is not finished and may still contain bugs\n",
    "\n",
    "# # The algorithm is implemented only for the case d=2\n",
    "# def saddle_Algorithm():\n",
    "#     t = 0\n",
    "#     beta = np.array([0, 0])\n",
    "#     s = np.array([0,0])\n",
    "\n",
    "#     arr_t = [t]\n",
    "#     arr_beta = [beta]\n",
    "#     arr_s = [s]\n",
    "\n",
    "#     while (nabla_L(beta)[0] != 0) or (nabla_L(beta)[1] != 0) : #Will it ever be accurate enough to have zero loss?\n",
    "#         print(\"Starting beta - \", beta)\n",
    "#         beta0_is0 = False\n",
    "#         beta1_is0 = False\n",
    "#         # Step one, add unstable coordinates to a set A\n",
    "#         A = []\n",
    "#         print(\"nabla_L - \", nabla_L(beta))\n",
    "#         for i in range (d):\n",
    "#             if(nabla_L(beta)[i] != 0):\n",
    "#                 A.append(i)\n",
    "\n",
    "#         print(\"Set A - \", A)\n",
    "\n",
    "#         # Step two, find the time at which a jump occurs\n",
    "#         epsilon = 0.001\n",
    "#         for i in A:\n",
    "#             inf_delta = math.inf\n",
    "#             delta = max((s[i] - 1) / nabla_L(beta)[i],(s[i] + 1) / nabla_L(beta)[i]) # s will only ever hit 1 or -1 in the future, the other is hit in the past\n",
    "#             inf_delta = min(inf_delta, delta)\n",
    "\n",
    "#         print(\"delta = \", inf_delta)\n",
    "#         # Step three, the time and new s vairables are updated\n",
    "#         t = t + inf_delta\n",
    "#         s = s - inf_delta * nabla_L(beta)\n",
    "#         arr_t.append(t)\n",
    "#         arr_s.append(s)\n",
    "#         print(\"t - \",t)\n",
    "#         print(\"s - \",s)\n",
    "#         print(\"=============\")\n",
    "\n",
    "#         # Step four, calculate next saddle that beta jumps to\n",
    "\n",
    "#         # So here we try some funky stuff, the argmin function needs discrete values, so we generate a bunch of values between -5 to 5 in increments of 0.001\n",
    "#         # We simply have to hope this is accurate enough and that the beta values do not go outside this range (this range can be tested in the graphs above)\n",
    "#         lowerbound, upperbound,precision = -2,2,0.01 \n",
    "#         possible_beta_vals = np.arange(lowerbound,upperbound+precision,precision)\n",
    "\n",
    "#         if(s[0] == 1):\n",
    "#             mask0 = (possible_beta_vals >= 0) \n",
    "#             possible_beta_0 = possible_beta_vals[mask0]\n",
    "#         elif(s[0] == -1):\n",
    "#             mask0 = (possible_beta_vals <= 0)\n",
    "#             possible_beta_0 = possible_beta_vals[mask0]\n",
    "#         else:\n",
    "#             possible_beta_0 = np.array([0]) #We set this manually cos of floating accuracy at the specific value 0, a mask at only 0 can fail\n",
    "#             beta[0] = 0\n",
    "#             beta0_is0 = True\n",
    "\n",
    "#         if(s[1] == 1):\n",
    "#             mask1 = (possible_beta_vals >= 0) \n",
    "#             possible_beta_1 = possible_beta_vals[mask1]\n",
    "#         elif(s[1] == -1):\n",
    "#             mask1 = (possible_beta_vals <= 0)\n",
    "#             possible_beta_1 = possible_beta_vals[mask1]\n",
    "#         else:\n",
    "#             possible_beta_1 = np.array([0]) #We set this manually cos of floating accuracy at the specific value 0, a mask at only 0 can fail\n",
    "#             beta[1] = 0\n",
    "#             beta1_is0 = True\n",
    "\n",
    "#         number_possible_beta_0 = len(possible_beta_0)\n",
    "#         number_possible_beta_1 = len(possible_beta_1)\n",
    "#         loss_vals = np.empty([number_possible_beta_0,number_possible_beta_1])\n",
    "\n",
    "#         for i in range(number_possible_beta_0):\n",
    "#             for j in range(number_possible_beta_1):\n",
    "#                 test_beta = np.array([possible_beta_0[i],possible_beta_1[j]])\n",
    "#                 loss_vals[i][j] = loss_L_beta(test_beta)\n",
    "\n",
    "#         argmin = np.argmin(loss_vals,axis=1)\n",
    "\n",
    "#         if(not beta0_is0 and not beta1_is0):\n",
    "#             beta = np.array([possible_beta_0[argmin[0]],possible_beta_1[argmin[1]]])\n",
    "#         elif(not beta0_is0):\n",
    "#             beta[0] = possible_beta_0[argmin[0]]\n",
    "#         elif(not beta1_is0):\n",
    "#             beta[1] = possible_beta_1[argmin[0]]\n",
    "\n",
    "#         arr_beta.append(beta)\n",
    "#         #print(beta) \n",
    "\n",
    "# saddle_Algorithm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(Assumption 1) - General Positions**\n",
    "\n",
    "For any $k \\leq \\min(n,d)$ and arbitrary signs $\\sigma_1, \\dots, \\sigma_k \\in \\{-1,1\\}$, the affine span of any $k$ points $\\sigma \\tilde{x}_{j_1}, \\ldots, \\sigma \\tilde{x}_{j_k}$ does not contain any element of the set $\\{ \\pm \\tilde{x_j}, j \\neq j_1, \\ldots, j_k\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_affine_span(X, k):\n",
    "    \"\"\"\n",
    "    Check if the dataset X satisfies the general position assumption.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy array): Dataset with shape (n, d), where n is the number of samples and d is the dimensionality.\n",
    "                     The ith column of X corresponds to the feature vector x_̃j in the assumption.\n",
    "    k (int): Number of points to check in the affine span. Must be <= min(n, d).\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the assumption holds, False otherwise.\n",
    "\n",
    "    Warning: \n",
    "    This function has exponential complexity in d and k. It is only suitable for small values of d and k.\n",
    "    \"\"\"\n",
    "    # Generate all combinations of k columns and their corresponding sign combinations\n",
    "    for indices in combinations(range(d), k): # all combinations of k columns\n",
    "        for signs in product([-1, 1], repeat=k): # all sign combinations\n",
    "            # Select k columns (corresponding to features) and apply sign changes\n",
    "            selected_columns = np.array([sign * X[:, idx] for sign, idx in zip(signs, indices)]).T  # Make array of all selected columns and their signs\n",
    "            \n",
    "            # Compute the affine span of the selected columns\n",
    "            # (Affine span is equivalent to checking linear independence after centering the points)\n",
    "            centered_columns = selected_columns - selected_columns.mean(axis=0) #Centering is necessary for checking the #affine# span not just the linear span\n",
    "            if np.linalg.matrix_rank(centered_columns) < k:\n",
    "                continue  # If points are linearly dependent, skip this combination\n",
    "            \n",
    "            # Now check if any other columns lie in the affine span\n",
    "            remaining_indices = [i for i in range(d) if i not in indices] # Columns not in the selected set\n",
    "            for idx in remaining_indices:\n",
    "                for sign in [-1, 1]:\n",
    "                    candidate_column = sign * X[:, idx]\n",
    "                    augmented_matrix = np.hstack([centered_columns, candidate_column[:, None] - selected_columns.mean(axis=0)])\n",
    "                    if np.linalg.matrix_rank(augmented_matrix) == k:\n",
    "                        # If the rank doesn't increase, the point lies in the affine span\n",
    "                        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = min(d, n)  # Minimum of d and n for general positions\n",
    "\n",
    "for i in range(1, k+1):\n",
    "    if not check_affine_span(samples_x, i):\n",
    "        print(\"General position assumption is not satisfied for k = \", i)\n",
    "    else:\n",
    "        print(\"General position assumption is satisfied for k = \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating graphs for Phi as Alpha goes to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values\n",
    "x = np.linspace(-10, 10, 500)  # 500 points for smooth curve\n",
    "alpha = 5\n",
    "# Compute y values\n",
    "y_1 = 1/2*(x * np.arcsinh(x / alpha**2) - np.sqrt(x**2 + alpha**4) + alpha**2)\n",
    "\n",
    "alpha = 1\n",
    "y_2 = 1/2*(x * np.arcsinh(x / alpha**2) - np.sqrt(x**2 + alpha**4) + alpha**2)\n",
    "\n",
    "alpha = 0.001\n",
    "y_3 = 1/2*(x * np.arcsinh(x / alpha**2) - np.sqrt(x**2 + alpha**4) + alpha**2)\n",
    "# Plot the function\n",
    "font = 26\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(24, 6))\n",
    "ax1.plot(x, y_1)\n",
    "ax1.grid(True)\n",
    "ax1.set_title(r'Plot $\\phi_\\alpha(x)$ for $\\alpha = 5$',fontsize=font)\n",
    "\n",
    "ax2.plot(x, y_2)\n",
    "ax2.grid(True)\n",
    "ax2.set_title(r'Plot $\\phi_\\alpha(x)$ for $\\alpha = 1$',fontsize=font)\n",
    "\n",
    "ax3.plot(x, y_3)\n",
    "ax3.grid(True)\n",
    "ax3.set_title(r'Plot $\\phi_\\alpha(x)$ for $\\alpha = 0.001$',fontsize=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
